以下是关于 **ChatGLM 模型系列** 的全部论文及其对应链接，涵盖从基础架构到最新版本的完整技术报告：

---

### **1. GLM 基础架构论文**
- **标题**: *GLM: General Language Model Pretraining with Autoregressive Blank Infilling*  
- **作者**: 清华大学团队  
- **内容**: 提出 GLM 预训练框架，结合自编码和自回归优势，支持 NLU 和生成任务。  
- **链接**: [arXiv:2103.10360](https://arxiv.org/abs/2103.10360)   

---

### **2. GLM-130B 技术报告**  
- **标题**: *GLM-130B: An Open Bilingual Pre-trained Model*  
- **作者**: 智谱AI & 清华大学  
- **内容**: 开源 1300 亿参数双语模型，训练稳定性和量化优化方法。  
- **链接**: [arXiv:2210.02414](https://arxiv.org/abs/2210.02414)   

---

### **3. ChatGLM-6B 技术报告**  
- **标题**: *ChatGLM-6B: An Open Bilingual Dialogue Language Model*  
- **内容**: 62 亿参数中英对话模型，支持本地部署和高效微调。  
- **链接**: [GitHub 仓库](https://github.com/THUDM/ChatGLM-6B)   

---

### **4. GLM-4 系列技术报告**  
- **标题**: *ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools*  
- **机构**: 智谱AI  
- **内容**: 介绍 GLM-4、GLM-4-Air 和 GLM-4-9B，支持工具调用和长上下文。  
- **链接**: [arXiv:2406.12793](https://arxiv.org/abs/2406.12793)   

---

### **5. 其他相关论文**  
- **训练稳定性研究**: 嵌入梯度收缩和 DeepNorm 方法（见 GLM-130B 报告）  
- **多任务指导预训练**: 结合指令微调提升零样本能力（GLM-4 报告）  

---

### **资源汇总**  
- **GitHub**: [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)  
- **Hugging Face**: [ChatGLM 模型库](https://huggingface.co/THUDM)  
- **官方博客**: [ChatGLM 更新日志](https://chatglm.cn/blog)  

如需更详细的实验数据或部署指南，可参考上述论文的附录或官方文档。